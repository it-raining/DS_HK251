import logging
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.sql.functions import col

# --- C·∫§U H√åNH ---
# L∆∞u √Ω: D√πng port 8020 cho k·∫øt n·ªëi RPC (nh∆∞ trong docker-compose)
HDFS_INPUT_PATH = "hdfs://namenode:8020/data/smart_meter_cleaned"
MODEL_OUTPUT_PATH = "hdfs://namenode:8020/models/consumption_model"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # 1. Kh·ªüi t·∫°o Spark Session (Batch Mode)
    spark = SparkSession.builder \
        .appName("SmartMeterTraining") \
        .config("spark.cores.max", "1") \
        .config("spark.executor.cores", "1") \
        .getOrCreate()
    
    logger.info("Training Job Started...")

    # 2. Load d·ªØ li·ªáu t·ª´ HDFS (Parquet)
    try:
        # ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu hi·ªán c√≥ trong Data Lake
        df = spark.read.parquet(HDFS_INPUT_PATH)
        
        # In ra s·ªë l∆∞·ª£ng b·∫£n ghi ƒë·ªÉ ki·ªÉm tra
        record_count = df.count()
        logger.info(f"Found {record_count} records for training.")
        
        if record_count == 0:
            logger.warning("No data found! Please wait for ingest_data.py to run for a while.")
            return
            
    except Exception as e:
        logger.error(f"‚ùå Error reading data from HDFS: {e}")
        logger.info("Tip: Ensure ingest_data.py is running and generating parquet files.")
        return

    # 3. Chu·∫©n b·ªã d·ªØ li·ªáu (Feature Engineering)
    # C·∫ßn s·ª≠a l·∫°i logic model ·ªü ƒë√¢y, hi·ªán ƒëang implement Model ƒë∆°n gi·∫£n: D·ª± ƒëo√°n C√¥ng su·∫•t (Power) d·ª±a tr√™n D√≤ng ƒëi·ªán (Current) v√† ƒêi·ªán √°p (Voltage) 
    # Ch·ªçn c√°c c·ªôt input (Features) v√† c·ªôt target (Label)
    feature_cols = ["voltage", "current", "power_factor", "frequency"]
    label_col = "power" # active_power_kw

    # Lo·∫°i b·ªè c√°c d√≤ng c√≥ gi√° tr·ªã null ƒë·ªÉ tr√°nh l·ªói training
    train_data = df.select(feature_cols + [label_col]).dropna()

    # VectorAssembler: Gom c√°c c·ªôt features th√†nh 1 vector duy nh·∫•t (Spark ML y√™u c·∫ßu)
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

    # 4. ƒê·ªãnh nghƒ©a Model (Linear Regression)
    lr = LinearRegression(featuresCol="features", labelCol=label_col)

    # T·∫°o Pipeline: Gom b∆∞·ªõc x·ª≠ l√Ω vector v√† b∆∞·ªõc training l·∫°i
    pipeline = Pipeline(stages=[assembler, lr])

    # 5. Hu·∫•n luy·ªán Model
    logger.info("Training model...")
    model = pipeline.fit(train_data)

    # In ra c√°c h·ªá s·ªë c·ªßa model (Coefficients) ƒë·ªÉ xem n√≥ h·ªçc ƒë∆∞·ª£c g√¨
    lr_model = model.stages[-1]
    logger.info(f"Model Trained! Coefficients: {lr_model.coefficients} Intercept: {lr_model.intercept}")

    # 6. L∆∞u Model xu·ªëng HDFS
    # Cho ph√©p ghi ƒë√® (overwrite) ƒë·ªÉ c·∫≠p nh·∫≠t model m·ªõi nh·∫•t
    try:
        model.write().overwrite().save(MODEL_OUTPUT_PATH)
        logger.info(f"üíæ Model saved successfully to: {MODEL_OUTPUT_PATH}")
    except Exception as e:
        logger.error(f"‚ùå Failed to save model: {e}")

    spark.stop()

if __name__ == "__main__":
    main()