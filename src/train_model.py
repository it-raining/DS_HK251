import logging
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.sql.functions import col

# --- Cáº¤U HÃŒNH ---
# LÆ°u Ã½: DÃ¹ng port 8020 cho káº¿t ná»‘i RPC (nhÆ° trong docker-compose)
HDFS_INPUT_PATH = "hdfs://namenode:8020/data/smart_meter_cleaned"
MODEL_OUTPUT_PATH = "hdfs://namenode:8020/models/consumption_model"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # 1. Khá»Ÿi táº¡o Spark Session (Batch Mode)
    spark = SparkSession.builder \
        .appName("SmartMeterTraining") \
        .getOrCreate()
    
    logger.info("ğŸš€ Training Job Started...")

    # 2. Load dá»¯ liá»‡u tá»« HDFS (Parquet)
    try:
        # Äá»c toÃ n bá»™ dá»¯ liá»‡u hiá»‡n cÃ³ trong Data Lake
        df = spark.read.parquet(HDFS_INPUT_PATH)
        
        # In ra sá»‘ lÆ°á»£ng báº£n ghi Ä‘á»ƒ kiá»ƒm tra
        record_count = df.count()
        logger.info(f"ğŸ“Š Found {record_count} records for training.")
        
        if record_count == 0:
            logger.warning("âš ï¸ No data found! Please wait for ingest_data.py to run for a while.")
            return
            
    except Exception as e:
        logger.error(f"âŒ Error reading data from HDFS: {e}")
        logger.info("Tip: Ensure ingest_data.py is running and generating parquet files.")
        return

    # 3. Chuáº©n bá»‹ dá»¯ liá»‡u (Feature Engineering)
    # Cáº§n sá»­a láº¡i logic model á»Ÿ Ä‘Ã¢y, hiá»‡n Ä‘ang implement Model Ä‘Æ¡n giáº£n: Dá»± Ä‘oÃ¡n CÃ´ng suáº¥t (Power) dá»±a trÃªn DÃ²ng Ä‘iá»‡n (Current) vÃ  Äiá»‡n Ã¡p (Voltage) 
    # Chá»n cÃ¡c cá»™t input (Features) vÃ  cá»™t target (Label)
    feature_cols = ["voltage", "current", "power_factor", "frequency"]
    label_col = "power" # active_power_kw

    # Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null Ä‘á»ƒ trÃ¡nh lá»—i training
    train_data = df.select(feature_cols + [label_col]).dropna()

    # VectorAssembler: Gom cÃ¡c cá»™t features thÃ nh 1 vector duy nháº¥t (Spark ML yÃªu cáº§u)
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

    # 4. Äá»‹nh nghÄ©a Model (Linear Regression)
    lr = LinearRegression(featuresCol="features", labelCol=label_col)

    # Táº¡o Pipeline: Gom bÆ°á»›c xá»­ lÃ½ vector vÃ  bÆ°á»›c training láº¡i
    pipeline = Pipeline(stages=[assembler, lr])

    # 5. Huáº¥n luyá»‡n Model
    logger.info("ğŸ‹ï¸ Training model...")
    model = pipeline.fit(train_data)

    # In ra cÃ¡c há»‡ sá»‘ cá»§a model (Coefficients) Ä‘á»ƒ xem nÃ³ há»c Ä‘Æ°á»£c gÃ¬
    lr_model = model.stages[-1]
    logger.info(f"âœ… Model Trained! Coefficients: {lr_model.coefficients} Intercept: {lr_model.intercept}")

    # 6. LÆ°u Model xuá»‘ng HDFS
    # Cho phÃ©p ghi Ä‘Ã¨ (overwrite) Ä‘á»ƒ cáº­p nháº­t model má»›i nháº¥t
    try:
        model.write().overwrite().save(MODEL_OUTPUT_PATH)
        logger.info(f"ğŸ’¾ Model saved successfully to: {MODEL_OUTPUT_PATH}")
    except Exception as e:
        logger.error(f"âŒ Failed to save model: {e}")

    spark.stop()

if __name__ == "__main__":
    main()