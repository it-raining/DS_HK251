import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, to_date, coalesce, lit
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType

# --- Cáº¤U HÃŒNH ---
KAFKA_BOOTSTRAP_SERVERS = "kafka1:29092,kafka2:29092"
KAFKA_TOPIC = "smart-meter-data"
HDFS_PATH = "hdfs://namenode:8020/data/smart_meter_cleaned" # NÆ¡i lÆ°u dá»¯ liá»‡u sáº¡ch
CHECKPOINT_PATH = "hdfs://namenode:8020/checkpoints/ingest_job" # Báº¯t buá»™c cho Streaming

# Thiáº¿t láº­p logging Ä‘á»ƒ dá»… theo dÃµi
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    # 1. Khá»Ÿi táº¡o Spark Session
    spark = SparkSession.builder \
        .appName("SmartMeterIngestion") \
        .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH) \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .config("spark.cores.max", "1") \
        .config("spark.executor.cores", "1") \
        .getOrCreate()

    logger.info("Spark Session started successfully")

    # 2. Äá»‹nh nghÄ©a Schema (Khá»›p 100% vá»›i JSON tá»« generator.py)
    # Cáº¥u trÃºc lá»“ng nhau (Nested Structure)
    json_schema = StructType([
        StructField("meter_id", StringType(), True),
        StructField("timestamp", StringType(), True), 
        StructField("location", StructType([
            StructField("city", StringType(), True),
            StructField("district", StringType(), True),
            StructField("gps", StringType(), True)
        ])),
        StructField("measurements", StructType([
            StructField("voltage_v", DoubleType(), True),
            StructField("current_a", DoubleType(), True),
            StructField("active_power_kw", DoubleType(), True),
            StructField("power_factor", DoubleType(), True),
            StructField("frequency_hz", DoubleType(), True),
            StructField("total_energy_kwh", DoubleType(), True)
        ])),
        StructField("status", StructType([
            StructField("code", IntegerType(), True),
            StructField("message", StringType(), True),
            StructField("battery_level", IntegerType(), True)
        ]))
    ])

    # 3. Äá»c dá»¯ liá»‡u tá»« Kafka (ReadStream)
    raw_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", "earliest") \
        .option("failOnDataLoss", "false") \
        .load()

    # 4. Parse JSON & Flatten (LÃ m pháº³ng dá»¯ liá»‡u)
    # Chuyá»ƒn cá»™t 'value' (binary) thÃ nh String rá»“i parse JSON
    parsed_stream = raw_stream.select(
        from_json(col("value").cast("string"), json_schema).alias("data")
    ).select("data.*") # Bung cÃ¡c trÆ°á»ng ra

    # 5. TIá»€N Xá»¬ LÃ (PRE-PROCESSING / CLEANING) ðŸ§¹
    cleaned_stream = parsed_stream \
        .withColumn("event_time", col("timestamp").cast(TimestampType())) \
        .withColumn("date", to_date(col("event_time"))) \
        .select(
            col("meter_id"),
            col("event_time"),
            col("date"), # DÃ¹ng Ä‘á»ƒ partition
            col("location.district"),
            col("measurements.voltage_v").alias("voltage"),
            col("measurements.current_a").alias("current"),
            col("measurements.active_power_kw").alias("power"),
            col("measurements.power_factor").alias("power_factor"),
            col("measurements.frequency_hz").alias("frequency"),
            col("measurements.total_energy_kwh").alias("energy"),
            col("status.code").alias("status_code")
        )

    # --- Bá»˜ Lá»ŒC (FILTERING RULES) ---
    # Rule 1: Loáº¡i bá» dá»¯ liá»‡u Null á»Ÿ cÃ¡c trÆ°á»ng quan trá»ng (Meter ID, Time)
    filtered_stream = cleaned_stream.filter(col("meter_id").isNotNull() & col("event_time").isNotNull())
    
    # Rule 2: Logic lá»c giÃ¡ trá»‹ rÃ¡c
    # LÆ°u Ã½: Generator cÃ³ thá»ƒ táº¯t metric (gá»­i null). 
    # DÃ¹ng coalesce(col, 0) Ä‘á»ƒ coi nhÆ° báº±ng 0 náº¿u null, trÃ¡nh drop máº¥t dÃ²ng data
    filtered_stream = filtered_stream.filter(
        (coalesce(col("voltage"), lit(1)) > 0) & 
        (coalesce(col("power"), lit(0)) >= 0)
    )

    # 6. Ghi xuá»‘ng HDFS (WriteStream)
    # Äá»‹nh dáº¡ng Parquet: Tá»‘i Æ°u cho lÆ°u trá»¯ vÃ  truy váº¥n sau nÃ y
    query = filtered_stream.writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", HDFS_PATH) \
        .option("checkpointLocation", CHECKPOINT_PATH) \
        .partitionBy("date") \
        .trigger(processingTime="1 minute") \
        .start()

    logger.info(f"Streaming to HDFS started. Path: {HDFS_PATH}")
    logger.info(">>> Partitioning by 'date'. Trigger interval: 1 minute.")
    query.awaitTermination()

if __name__ == "__main__":
    main()