import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, expr, when
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType

# --- C·∫§U H√åNH ---
KAFKA_BOOTSTRAP_SERVERS = "kafka1:29092,kafka2:29092"
KAFKA_TOPIC = "smart-meter-data"
HDFS_PATH = "hdfs://namenode:8020/data/smart_meter_cleaned" # N∆°i l∆∞u d·ªØ li·ªáu s·∫°ch
CHECKPOINT_PATH = "hdfs://namenode:8020/checkpoints/ingest_job" # B·∫Øt bu·ªôc cho Streaming

# Thi·∫øt l·∫≠p logging ƒë·ªÉ d·ªÖ theo d√µi
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # 1. Kh·ªüi t·∫°o Spark Session
    spark = SparkSession.builder \
        .appName("SmartMeterIngestion") \
        .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH) \
        .config("spark.cores.max", "1") \
        .config("spark.executor.cores", "1") \
        .getOrCreate()

    logger.info("Spark Session started successfully")

    # 2. ƒê·ªãnh nghƒ©a Schema (Kh·ªõp 100% v·ªõi JSON t·ª´ generator.py)
    # C·∫•u tr√∫c l·ªìng nhau (Nested Structure)
    json_schema = StructType([
        StructField("meter_id", StringType(), True),
        StructField("timestamp", StringType(), True), # Nh·∫≠n l√† String tr∆∞·ªõc, cast sang Timestamp sau
        StructField("location", StructType([
            StructField("city", StringType(), True),
            StructField("district", StringType(), True),
            StructField("gps", StringType(), True)
        ])),
        StructField("measurements", StructType([
            StructField("voltage_v", DoubleType(), True),
            StructField("current_a", DoubleType(), True),
            StructField("active_power_kw", DoubleType(), True),
            StructField("power_factor", DoubleType(), True),
            StructField("frequency_hz", DoubleType(), True),
            StructField("total_energy_kwh", DoubleType(), True)
        ])),
        StructField("status", StructType([
            StructField("code", IntegerType(), True),
            StructField("message", StringType(), True),
            StructField("battery_level", IntegerType(), True)
        ]))
    ])

    # 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka (ReadStream)
    raw_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", "earliest") \
        .load()

    # 4. Parse JSON & Flatten (L√†m ph·∫≥ng d·ªØ li·ªáu)
    # Chuy·ªÉn c·ªôt 'value' (binary) th√†nh String r·ªìi parse JSON
    parsed_stream = raw_stream.select(
        from_json(col("value").cast("string"), json_schema).alias("data")
    ).select("data.*") # Bung c√°c tr∆∞·ªùng ra

    # 5. TI·ªÄN X·ª¨ L√ù (PRE-PROCESSING / CLEANING) üßπ
    cleaned_stream = parsed_stream \
        .withColumn("event_time", col("timestamp").cast(TimestampType())) \
        .select(
            col("meter_id"),
            col("event_time"),
            col("location.district"),
            col("measurements.voltage_v").alias("voltage"),
            col("measurements.current_a").alias("current"),
            col("measurements.active_power_kw").alias("power"),
            col("measurements.power_factor").alias("power_factor"),
            col("measurements.frequency_hz").alias("frequency"),
            col("measurements.total_energy_kwh").alias("energy"),
            col("status.code").alias("status_code")
        )

    # --- B·ªò L·ªåC (FILTERING RULES) ---
    # Rule 1: Lo·∫°i b·ªè d·ªØ li·ªáu Null ·ªü c√°c tr∆∞·ªùng quan tr·ªçng (Meter ID, Time)
    filtered_stream = cleaned_stream.filter(col("meter_id").isNotNull() & col("event_time").isNotNull())
    
    # (c√≥ th·ªÉ gi·ªØ l·∫°i ƒë·ªÉ ph√¢n t√≠ch l·ªói, nh∆∞ng ·ªü ƒë√¢y gi·∫£ s·ª≠ ta c·∫ßn data s·∫°ch ƒë·ªÉ train model d·ª± ƒëo√°n ti√™u th·ª•)
    # filtered_stream = filtered_stream.filter(col("status_code") == 0)

    # Rule 3: Ki·ªÉm tra mi·ªÅn gi√° tr·ªã (Sanity Check)
    # Voltage ph·∫£i > 0, Power >= 0
    filtered_stream = filtered_stream.filter((col("voltage") > 0) & (col("power") >= 0))

    # 6. Ghi xu·ªëng HDFS (WriteStream)
    # ƒê·ªãnh d·∫°ng Parquet: T·ªëi ∆∞u cho l∆∞u tr·ªØ v√† truy v·∫•n sau n√†y
    query = filtered_stream.writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", HDFS_PATH) \
        .option("checkpointLocation", CHECKPOINT_PATH) \
        .trigger(processingTime="10 seconds") \
        .start()

    logger.info(f"Streaming to HDFS started. Path: {HDFS_PATH}")
    query.awaitTermination()

if __name__ == "__main__":
    main()